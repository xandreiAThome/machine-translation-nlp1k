{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xandreiAThome/machine-translation-nlp1k/blob/main/nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f26ef694",
      "metadata": {
        "id": "f26ef694"
      },
      "source": [
        "# Neural Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "dItCMmUTahpU"
      },
      "id": "dItCMmUTahpU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the aligned verses from the tsv, clean the string from any non alphabetic characters. Remove any verses that have no verse for either of the two language, and use the class from the datasets library to structure the data and be ready for training."
      ],
      "metadata": {
        "id": "8uSrJrwzkTJR"
      },
      "id": "8uSrJrwzkTJR"
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "\n",
        "def clean_string(input_string):\n",
        "    cleaned = re.sub(r\"[^\\p{L}\\s]\", \"\", input_string.strip().lower())\n",
        "    return cleaned\n",
        "\n",
        "def process(example):\n",
        "    src = example[\"src\"].strip()\n",
        "    tgt = example[\"tgt\"].strip()\n",
        "\n",
        "    # skip invalid pairs\n",
        "    if src.lower() == \"<no verse>\" or tgt.lower() == \"<no verse>\":\n",
        "        return {\"src\": None, \"tgt\": None}\n",
        "\n",
        "    return {\n",
        "        \"src\": clean_string(src),\n",
        "        \"tgt\": clean_string(tgt),\n",
        "    }"
      ],
      "metadata": {
        "id": "WROb-ouqeXxv"
      },
      "id": "WROb-ouqeXxv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_lang = \"Bikolano\"\n",
        "target_lang = \"Tagalog\""
      ],
      "metadata": {
        "id": "Crzo9qA_gok5"
      },
      "id": "Crzo9qA_gok5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files=\"data/dataset/Bikolano_Tagalog_Parallel.tsv\",\n",
        "    delimiter=\"\\t\",\n",
        ")\n",
        "\n",
        "dataset = dataset[\"train\"].select_columns([src_lang, target_lang])\n",
        "dataset = dataset.rename_columns({src_lang: \"src\", target_lang: \"tgt\"})\n",
        "\n",
        "# Get initial dataset length\n",
        "initial_dataset_length = len(dataset)\n",
        "\n",
        "dataset = dataset.map(process)\n",
        "\n",
        "# remove rows with None (invalid)\n",
        "dataset = dataset.filter(lambda x: x[\"src\"] is not None and x[\"tgt\"] is not None)\n",
        "\n",
        "# Calculate skipped verses\n",
        "skipped = initial_dataset_length - len(dataset)\n",
        "print(f\"skipped verses: {skipped}\")"
      ],
      "metadata": {
        "id": "yN0f3liyd_do"
      },
      "id": "yN0f3liyd_do",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets look at the first 5 aligned verses"
      ],
      "metadata": {
        "id": "-K23zGN6hB9i"
      },
      "id": "-K23zGN6hB9i"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "888bafc8"
      },
      "source": [
        "display(dataset[:5])"
      ],
      "id": "888bafc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Trainer\n",
        "We will use facebook's No Language Left Behind Model as the base model to fine tune using our dataset. It is performant even on low resource languages thats why our group decided to use it."
      ],
      "metadata": {
        "id": "Mh8YswZ0pN3S"
      },
      "id": "Mh8YswZ0pN3S"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "y5H6fdpNjLfK"
      },
      "id": "y5H6fdpNjLfK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    model_inputs = tokenizer(batch[\"src\"], truncation=True, max_length=128)\n",
        "    labels = tokenizer(batch[\"tgt\"], truncation=True, max_length=128).input_ids\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "CKsz_bS1kCUx"
      },
      "id": "CKsz_bS1kCUx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us split the training data to also have a dataset for evaluation after training."
      ],
      "metadata": {
        "id": "3OF6P_g6kNPf"
      },
      "id": "3OF6P_g6kNPf"
    },
    {
      "cell_type": "code",
      "source": [
        "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "train_data = split[\"train\"]\n",
        "eval_data = split[\"test\"]"
      ],
      "metadata": {
        "id": "CNpweOlolIlq"
      },
      "id": "CNpweOlolIlq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments,\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./nllb-bcl-tgl\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=8,\n",
        "    eval_strategy=\"epoch\", # Changed from evaluate_during_training\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=2,  # effective batch size = 8\n",
        "    weight_decay=0.01,\n",
        "    predict_with_generate=True,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "ZTtqUFUSlpLR"
      },
      "id": "ZTtqUFUSlpLR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model(\"./nllb-bcl-tgl\")\n",
        "tokenizer.save_pretrained(\"./nllb-bcl-tgl\")\n"
      ],
      "metadata": {
        "id": "a95sG6HVnhzP"
      },
      "id": "a95sG6HVnhzP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}