{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2d66f7",
   "metadata": {},
   "source": [
    "# Statistical Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da00ec",
   "metadata": {},
   "source": [
    "### Plan\n",
    "- Preprocess dataset\n",
    "- Train Model\n",
    "- Evaluate Performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6936a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2025.11.3)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (3.9.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: dill in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.4.0)\n",
      "Collecting eflomal (from -r requirements.txt (line 5))\n",
      "Collecting eflomal (from -r requirements.txt (line 5))\n",
      "  Downloading eflomal-2.0.0.tar.gz (132 kB)\n",
      "  Downloading eflomal-2.0.0.tar.gz (132 kB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25hRequirement already satisfied: click in ./.venv/lib/python3.13/site-packages (from nltk->-r requirements.txt (line 2)) (8.3.0)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.13/site-packages (from nltk->-r requirements.txt (line 2)) (1.5.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from nltk->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 3)) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "\bdone\n",
      "\u001b[?25hRequirement already satisfied: click in ./.venv/lib/python3.13/site-packages (from nltk->-r requirements.txt (line 2)) (8.3.0)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.13/site-packages (from nltk->-r requirements.txt (line 2)) (1.5.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from nltk->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 3)) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Collecting Cython (from eflomal->-r requirements.txt (line 5))\n",
      "  Using cached cython-3.2.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
      "Using cached cython-3.2.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Building wheels for collected packages: eflomal\n",
      "  Building wheel for eflomal (pyproject.toml) ... \u001b[?25lCollecting Cython (from eflomal->-r requirements.txt (line 5))\n",
      "  Using cached cython-3.2.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
      "Using cached cython-3.2.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Building wheels for collected packages: eflomal\n",
      "  Building wheel for eflomal (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Created wheel for eflomal: filename=eflomal-2.0.0-cp313-cp313-linux_x86_64.whl size=382408 sha256=04035a4b604e9e250db76f10c7eb15bae18c66e20ecc8da9c2e0e2957c533d04\n",
      "  Stored in directory: /home/xandreiathome/.cache/pip/wheels/fd/4c/68/e92bd53571654de704f9563d422b38f92d7ae99241ec9a1516\n",
      "Successfully built eflomal\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for eflomal: filename=eflomal-2.0.0-cp313-cp313-linux_x86_64.whl size=382408 sha256=04035a4b604e9e250db76f10c7eb15bae18c66e20ecc8da9c2e0e2957c533d04\n",
      "  Stored in directory: /home/xandreiathome/.cache/pip/wheels/fd/4c/68/e92bd53571654de704f9563d422b38f92d7ae99241ec9a1516\n",
      "Successfully built eflomal\n",
      "Installing collected packages: Cython, eflomal\n",
      "\u001b[?25lInstalling collected packages: Cython, eflomal\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [eflomal]m1/2\u001b[0m [eflomal]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Cython-3.2.1 eflomal-2.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [eflomal]m1/2\u001b[0m [eflomal]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Cython-3.2.1 eflomal-2.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595bfe96",
   "metadata": {},
   "source": [
    "Based on the Language Clustering done in MCO2 we have chosen to train a Machine Translation Model for these pairs of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117047d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "language_pairs_file = {\n",
    "    \"Bikolano_Pangasinan\": \"Bikolano_Pangasinan_Parallel.tsv\",\n",
    "    \"Bikolano_Tagalog\": \"Bikolano_Tagalog_Parallel.tsv\",\n",
    "    \"Ivatan_Pangasinan\": \"Ivatan_Pangasinan_Parallel.tsv\",\n",
    "    \"Ivatan_Yami\": \"Ivatan_Yami_Parallel.tsv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a61c45",
   "metadata": {},
   "source": [
    "Clean up verses, tokenize the words and create the aligned verses for the two target languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41bd63bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Bikolano_Tagalog_Parallel.tsv...\n",
      "Source language: Bikolano\n",
      "Target language: Tagalog\n",
      "\n",
      "Processed 33900 verses from data/dataset/Bikolano_Tagalog_Parallel.tsv\n",
      "  Valid aligned pairs: 33873\n",
      "  Skipped (missing/empty verses): 27\n",
      "\n",
      "Loaded 33873 valid aligned pairs\n",
      "First source sentence tokens: ['si', 'adan', 'iyo', 'an', 'ama', 'ni', 'set', 'asin', 'si', 'set', 'iyo', 'an', 'ama', 'ni', 'enos', 'na', 'ama', 'ni', 'kenan']\n",
      "First target sentence tokens: ['sina', 'adan', 'set', 'enos']\n",
      "Saved 33873 aligned pairs to:\n",
      "  Source: data/aligned/Bikolano_Tagalog_source.txt\n",
      "  Target: data/aligned/Bikolano_Tagalog_target.txt\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import preprocess_parallel_tsv, save_aligned_corpus\n",
    "\n",
    "# Download NLTK punkt tokenizer if not already present\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "data_path = Path(\"data/dataset\")\n",
    "# Preprocess the parallel TSV file\n",
    "tsv_file = language_pairs_file[\"Bikolano_Tagalog\"]\n",
    "# get the src and target column name\n",
    "source_col, target_col = tsv_file.replace(\".tsv\", \"\").replace(\"data/\", \"\").split(\"_Parallel\")[0].split(\"_\")\n",
    "\n",
    "print(f\"Preprocessing {tsv_file}...\")\n",
    "print(f\"Source language: {source_col}\")\n",
    "print(f\"Target language: {target_col}\\n\")\n",
    "\n",
    "source_tokens, target_tokens = preprocess_parallel_tsv(\n",
    "    data_path / tsv_file, \n",
    "    source_col, \n",
    "    target_col,\n",
    "    output_dir=\"data/aligned\",\n",
    "    prefix=\"en_tl_\"\n",
    ")\n",
    "\n",
    "print(f\"\\nLoaded {len(source_tokens)} valid aligned pairs\")\n",
    "if source_tokens:\n",
    "    print(f\"First source sentence tokens: {source_tokens[0]}\")\n",
    "    print(f\"First target sentence tokens: {target_tokens[0]}\")\n",
    "\n",
    "# Save aligned corpus\n",
    "save_aligned_corpus(source_tokens, target_tokens, output_dir=\"data/aligned\", prefix=f\"{source_col}_{target_col}_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b970434",
   "metadata": {},
   "source": [
    "## Training the Machine Translation Model\n",
    "For this we will use the nltk IBMModels to train the parallel corpus that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca73f5",
   "metadata": {},
   "source": [
    "### Training IBMModel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae48856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 33873 AlignedSent objects.\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import AlignedSent\n",
    "aligned_verses = []\n",
    "for i in range(len(source_tokens)):\n",
    "    # Make sure neither verse is empty\n",
    "    if source_tokens[i] and target_tokens[i]:\n",
    "        # Note the order: AlignedSent(target_tokens, source_tokens)\n",
    "        aligned_verses.append(\n",
    "            AlignedSent(target_tokens[i], source_tokens[i])\n",
    "        )\n",
    "\n",
    "print(f\"Created {len(aligned_verses)} AlignedSent objects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c90f750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training IBM Model 1...\n",
      "IBM Model 1 translation table saved to data/models/ibm_model_1.pkl\n",
      "IBM Model 1 translation table saved to data/models/ibm_model_1.pkl\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import  IBMModel1\n",
    "from pathlib import Path\n",
    "import dill as pickle\n",
    "\n",
    "print(\"\\nTraining IBM Model 1...\")\n",
    "\n",
    "ibm1 = IBMModel1(aligned_verses, 5)\n",
    "\n",
    "# Save the translation table (can't pickle the model directly due to lambda functions)\n",
    "model1_path = Path(\"data/models/ibm_model_1.pkl\")\n",
    "model1_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(\"t_table.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ibm1, f)\n",
    "print(f\"IBM Model 1 translation table saved to {model1_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b9922",
   "metadata": {},
   "source": [
    "Lets test some common words and their equivalent word in the other language to see the probability if the source word will translate to the target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24296565843459514\n",
      "0.8265653837415294\n",
      "0.8772253051267173\n",
      "0.6107352610070307\n",
      "0.8147684148212581\n"
     ]
    }
   ],
   "source": [
    "print(ibm1.translation_table[\"sila\"][\"sinda\"])\n",
    "print(ibm1.translation_table[\"bahay\"][\"harong\"])\n",
    "print(ibm1.translation_table[\"apoy\"][\"kalayo\"])\n",
    "print(ibm1.translation_table[\"pagibig\"][\"pagkamoot\"])\n",
    "print(ibm1.translation_table[\"araw\"][\"aldaw\"])\n",
    "print(ibm1.translation_table[\"anak\"][\"aki\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259a0cf",
   "metadata": {},
   "source": [
    "### Training IBMModel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2a0b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training IBM Model 2...\n",
      "IBM Model 2 saved to data/models/ibm_model_2.pkl\n",
      "IBM Model 2 saved to data/models/ibm_model_2.pkl\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import IBMModel2\n",
    "import dill as pickle\n",
    "\n",
    "print(\"\\nTraining IBM Model 2...\")\n",
    "ibm2 = IBMModel2(aligned_verses, 5)\n",
    "\n",
    "# Save the trained IBM Model 2\n",
    "model2_path = Path(\"data/models/ibm_model_2.pkl\")\n",
    "model2_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(model2_path, \"wb\") as f:\n",
    "    pickle.dump(ibm2.translation_table, f)\n",
    "print(f\"IBM Model 2 saved to {model2_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc77098",
   "metadata": {},
   "source": [
    "Lets test the earlier words using IBMModel2 this time\n",
    "\n",
    "As we can see some words had a higher increase in probability, while some only increased a few percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22b69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30840326048144967\n",
      "0.948142201752353\n",
      "0.9363940892990702\n",
      "0.6399685190164236\n",
      "0.953474926632886\n"
     ]
    }
   ],
   "source": [
    "print(ibm2.translation_table[\"sila\"][\"sinda\"])\n",
    "print(ibm2.translation_table[\"bahay\"][\"harong\"])\n",
    "print(ibm2.translation_table[\"apoy\"][\"kalayo\"])\n",
    "print(ibm2.translation_table[\"pagibig\"][\"pagkamoot\"])\n",
    "print(ibm2.translation_table[\"araw\"][\"aldaw\"])\n",
    "print(ibm2.translation_table[\"anak\"][\"aki\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfde9ae",
   "metadata": {},
   "source": [
    "### Training IBMModel3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6074ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training IBM Model 3...\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import IBMModel3\n",
    "import dill as pickle\n",
    "\n",
    "print(\"\\nTraining IBM Model 3...\")\n",
    "ibm3 = IBMModel3(aligned_verses, 5)\n",
    "\n",
    "# Save the trained IBM Model 3\n",
    "model3_path = Path(\"data/models/ibm_model_3.pkl\")\n",
    "model3_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(model3_path, \"wb\") as f:\n",
    "    pickle.dump(ibm3.translation_table, f)\n",
    "print(f\"IBM Model 3 saved to {model3_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
