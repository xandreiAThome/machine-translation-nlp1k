{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d756c757",
   "metadata": {},
   "source": [
    "# Base NLLB Model Comparison\n",
    "\n",
    "Evaluate the base NLLB-200-distilled-600M model (without fine-tuning) on Yami → Tagalog translation.\n",
    "Results are saved to CSV for comparison with fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3a3c8",
   "metadata": {},
   "source": [
    "## Setup and Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sacrebleu\n",
    "import regex as re\n",
    "import torch\n",
    "import gc\n",
    "import statistics\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c5080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(input_string):\n",
    "    cleaned = re.sub(r\"[^\\p{L}\\s]\", \"\", input_string.strip().lower())\n",
    "    return cleaned\n",
    "\n",
    "def translate_nllb(text, model, tokenizer, device, max_length=128, tgt_lang=\"tgl_Latn\"):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        translated_tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "        )\n",
    "    \n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "print(\"Translation functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda25c9b",
   "metadata": {},
   "source": [
    "## Load Yami→Tagalog Validation Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = Path(\"data/validation/yami-tgl.tsv\")\n",
    "print(f\"Loading test data from: {TEST_DATA_PATH}\")\n",
    "test_df = pd.read_csv(TEST_DATA_PATH, sep='\\t')\n",
    "\n",
    "test_data = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    src = row['Yami']\n",
    "    ref = row['Tagalog']\n",
    "    \n",
    "    if pd.isna(src) or pd.isna(ref):\n",
    "        continue\n",
    "    if str(src).lower() == '<no verse>' or str(ref).lower() == '<no verse>':\n",
    "        continue\n",
    "    \n",
    "    src_text = str(src).strip()\n",
    "    ref_text = str(ref).strip()\n",
    "    \n",
    "    if src_text and ref_text:\n",
    "        src_cleaned = clean_string(src_text)\n",
    "        ref_cleaned = clean_string(ref_text)\n",
    "        \n",
    "        test_data.append({\n",
    "            'src': src_text,\n",
    "            'src_cleaned': src_cleaned,\n",
    "            'ref': ref_text,\n",
    "            'ref_cleaned': ref_cleaned\n",
    "        })\n",
    "\n",
    "print(f\"Prepared {len(test_data)} valid test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341f473",
   "metadata": {},
   "source": [
    "## Evaluate Base NLLB on Yami→Tagalog Validation Set (Direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc269057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base NLLB model (no fine-tuning)...\")\n",
    "base_model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "base_translations = []\n",
    "print(f\"Translating {len(test_data)} test pairs with base NLLB model...\")\n",
    "for i, item in enumerate(test_data):\n",
    "    if (i + 1) % max(1, len(test_data) // 10) == 0:\n",
    "        print(f\"  Progress: {i+1}/{len(test_data)}\")\n",
    "    \n",
    "    translation = translate_nllb(\n",
    "        item['src_cleaned'],\n",
    "        base_model,\n",
    "        base_tokenizer,\n",
    "        device,\n",
    "        tgt_lang=\"tgl_Latn\"\n",
    "    )\n",
    "    base_translations.append(translation)\n",
    "\n",
    "print(\"Base model translation complete\")\n",
    "\n",
    "del base_model\n",
    "gc.collect()\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "references = [item['ref_cleaned'] for item in test_data]\n",
    "bleu_base = sacrebleu.BLEU(smooth_method='exp')\n",
    "corpus_bleu_base = bleu_base.corpus_score(base_translations, [references])\n",
    "base_sentence_bleu_scores = [sacrebleu.BLEU(smooth_method='exp').sentence_score(hyp, [ref]).score for hyp, ref in zip(base_translations, references)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASE NLLB MODEL RESULTS (No Fine-tuning) - VALIDATION SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: facebook/nllb-200-distilled-600M\")\n",
    "print(f\"Target Language: Tagalog (tgl_Latn)\")\n",
    "print(f\"Test Pairs: {len(test_data)}\")\n",
    "print(f\"\\nCorpus BLEU Score: {corpus_bleu_base.score:.4f}\")\n",
    "print(f\"Mean Sentence BLEU: {statistics.mean(base_sentence_bleu_scores):.4f}\")\n",
    "print(f\"Median Sentence BLEU: {statistics.median(base_sentence_bleu_scores):.4f}\")\n",
    "print(f\"\\nBLEU Breakdown:\")\n",
    "print(f\"  BLEU-1: {corpus_bleu_base.precisions[0]:.4f}\")\n",
    "print(f\"  BLEU-2: {corpus_bleu_base.precisions[1]:.4f}\")\n",
    "print(f\"  BLEU-3: {corpus_bleu_base.precisions[2]:.4f}\")\n",
    "print(f\"  BLEU-4: {corpus_bleu_base.precisions[3]:.4f}\")\n",
    "print(f\"  Brevity Penalty: {corpus_bleu_base.bp:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aef7ba",
   "metadata": {},
   "source": [
    "## Load Yami→Tagalog Bible Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_dataset_path = Path(\"data/dataset/Tagalog_Yami_Parallel.tsv\")\n",
    "print(f\"Loading bible dataset from: {bible_dataset_path}\")\n",
    "bible_df = pd.read_csv(bible_dataset_path, sep='\\t')\n",
    "\n",
    "bible_with_yami = bible_df[(bible_df['Yami'] != '<no verse>') & (bible_df['Tagalog'] != '<no verse>')].copy()\n",
    "sample_size = min(200, len(bible_with_yami))\n",
    "bible_sample = bible_with_yami.sample(n=sample_size, random_state=42)\n",
    "\n",
    "bible_test_data = []\n",
    "for idx, row in bible_sample.iterrows():\n",
    "    src = row['Yami']\n",
    "    ref = row['Tagalog']\n",
    "    \n",
    "    if pd.isna(src) or pd.isna(ref):\n",
    "        continue\n",
    "    if str(src).lower() == '<no verse>' or str(ref).lower() == '<no verse>':\n",
    "        continue\n",
    "    \n",
    "    src_text = str(src).strip()\n",
    "    ref_text = str(ref).strip()\n",
    "    \n",
    "    if src_text and ref_text:\n",
    "        src_cleaned = clean_string(src_text)\n",
    "        ref_cleaned = clean_string(ref_text)\n",
    "        \n",
    "        bible_test_data.append({\n",
    "            'src': src_text,\n",
    "            'src_cleaned': src_cleaned,\n",
    "            'ref': ref_text,\n",
    "            'ref_cleaned': ref_cleaned\n",
    "        })\n",
    "\n",
    "print(f\"Prepared {len(bible_test_data)} valid bible verse pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4f629",
   "metadata": {},
   "source": [
    "## Evaluate Base NLLB on Yami→Tagalog Bible Dataset (Direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7493b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base NLLB model for Bible dataset evaluation...\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "base_bible_translations = []\n",
    "print(f\"Translating {len(bible_test_data)} Bible verses with base NLLB model...\")\n",
    "for i, item in enumerate(bible_test_data):\n",
    "    if (i + 1) % max(1, len(bible_test_data) // 10) == 0:\n",
    "        print(f\"  Progress: {i+1}/{len(bible_test_data)}\")\n",
    "    \n",
    "    translation = translate_nllb(\n",
    "        item['src_cleaned'],\n",
    "        base_model,\n",
    "        base_tokenizer,\n",
    "        device,\n",
    "        tgt_lang=\"tgl_Latn\"\n",
    "    )\n",
    "    base_bible_translations.append(translation)\n",
    "\n",
    "print(\"Base model Bible translation complete\")\n",
    "\n",
    "del base_model\n",
    "gc.collect()\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "bible_references = [item['ref_cleaned'] for item in bible_test_data]\n",
    "bleu_base_bible = sacrebleu.BLEU(smooth_method='exp')\n",
    "corpus_bleu_base_bible = bleu_base_bible.corpus_score(base_bible_translations, [bible_references])\n",
    "base_bible_sentence_bleu_scores = [sacrebleu.BLEU(smooth_method='exp').sentence_score(hyp, [ref]).score for hyp, ref in zip(base_bible_translations, bible_references)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASE NLLB MODEL RESULTS (No Fine-tuning) - BIBLE DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: facebook/nllb-200-distilled-600M\")\n",
    "print(f\"Target Language: Tagalog (tgl_Latn)\")\n",
    "print(f\"Test Pairs: {len(bible_test_data)}\")\n",
    "print(f\"\\nCorpus BLEU Score: {corpus_bleu_base_bible.score:.4f}\")\n",
    "print(f\"Mean Sentence BLEU: {statistics.mean(base_bible_sentence_bleu_scores):.4f}\")\n",
    "print(f\"Median Sentence BLEU: {statistics.median(base_bible_sentence_bleu_scores):.4f}\")\n",
    "print(f\"\\nBLEU Breakdown:\")\n",
    "print(f\"  BLEU-1: {corpus_bleu_base_bible.precisions[0]:.4f}\")\n",
    "print(f\"  BLEU-2: {corpus_bleu_base_bible.precisions[1]:.4f}\")\n",
    "print(f\"  BLEU-3: {corpus_bleu_base_bible.precisions[2]:.4f}\")\n",
    "print(f\"  BLEU-4: {corpus_bleu_base_bible.precisions[3]:.4f}\")\n",
    "print(f\"  Brevity Penalty: {corpus_bleu_base_bible.bp:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427053fd",
   "metadata": {},
   "source": [
    "## Load Pangasinan→Tagalog Bible Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd5a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pangasinan_dataset_path = Path(\"data/dataset/Tagalog_Pangasinan_Parallel.tsv\")\n",
    "print(f\"Loading Pangasinan-Tagalog dataset from: {pangasinan_dataset_path}\")\n",
    "pangasinan_df = pd.read_csv(pangasinan_dataset_path, sep='\\t')\n",
    "\n",
    "pangasinan_with_data = pangasinan_df[(pangasinan_df['Pangasinan'] != '<no verse>') & (pangasinan_df['Tagalog'] != '<no verse>')].copy()\n",
    "sample_size_pang = min(200, len(pangasinan_with_data))\n",
    "pangasinan_sample = pangasinan_with_data.sample(n=sample_size_pang, random_state=42)\n",
    "\n",
    "pangasinan_test_data = []\n",
    "for idx, row in pangasinan_sample.iterrows():\n",
    "    src = row['Pangasinan']\n",
    "    ref = row['Tagalog']\n",
    "    \n",
    "    if pd.isna(src) or pd.isna(ref):\n",
    "        continue\n",
    "    if str(src).lower() == '<no verse>' or str(ref).lower() == '<no verse>':\n",
    "        continue\n",
    "    \n",
    "    src_text = str(src).strip()\n",
    "    ref_text = str(ref).strip()\n",
    "    \n",
    "    if src_text and ref_text:\n",
    "        src_cleaned = clean_string(src_text)\n",
    "        ref_cleaned = clean_string(ref_text)\n",
    "        \n",
    "        pangasinan_test_data.append({\n",
    "            'src': src_text,\n",
    "            'src_cleaned': src_cleaned,\n",
    "            'ref': ref_text,\n",
    "            'ref_cleaned': ref_cleaned\n",
    "        })\n",
    "\n",
    "print(f\"Prepared {len(pangasinan_test_data)} valid Pangasinan-Tagalog pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937da57a",
   "metadata": {},
   "source": [
    "## Evaluate Base NLLB on Pangasinan→Tagalog Bible Dataset (Direct Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base NLLB model for Pangasinan-Tagalog evaluation...\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "pangasinan_translations = []\n",
    "print(f\"Translating {len(pangasinan_test_data)} Pangasinan sentences with base NLLB model...\")\n",
    "for i, item in enumerate(pangasinan_test_data):\n",
    "    if (i + 1) % max(1, len(pangasinan_test_data) // 10) == 0:\n",
    "        print(f\"  Progress: {i+1}/{len(pangasinan_test_data)}\")\n",
    "    \n",
    "    translation = translate_nllb(\n",
    "        item['src_cleaned'],\n",
    "        base_model,\n",
    "        base_tokenizer,\n",
    "        device,\n",
    "        tgt_lang=\"tgl_Latn\"\n",
    "    )\n",
    "    pangasinan_translations.append(translation)\n",
    "\n",
    "print(\"Base model Pangasinan-Tagalog translation complete\")\n",
    "\n",
    "del base_model\n",
    "gc.collect()\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "pangasinan_references = [item['ref_cleaned'] for item in pangasinan_test_data]\n",
    "bleu_pangasinan = sacrebleu.BLEU(smooth_method='exp')\n",
    "corpus_bleu_pangasinan = bleu_pangasinan.corpus_score(pangasinan_translations, [pangasinan_references])\n",
    "pangasinan_sentence_bleu_scores = [sacrebleu.BLEU(smooth_method='exp').sentence_score(hyp, [ref]).score for hyp, ref in zip(pangasinan_translations, pangasinan_references)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASE NLLB MODEL RESULTS - PANGASINAN→TAGALOG BIBLE (Direct Translation)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: facebook/nllb-200-distilled-600M\")\n",
    "print(f\"Source Language: Pangasinan\")\n",
    "print(f\"Target Language: Tagalog (tgl_Latn)\")\n",
    "print(f\"Test Pairs: {len(pangasinan_test_data)}\")\n",
    "print(f\"\\nCorpus BLEU Score: {corpus_bleu_pangasinan.score:.4f}\")\n",
    "print(f\"Mean Sentence BLEU: {statistics.mean(pangasinan_sentence_bleu_scores):.4f}\")\n",
    "print(f\"Median Sentence BLEU: {statistics.median(pangasinan_sentence_bleu_scores):.4f}\")\n",
    "print(f\"\\nBLEU Breakdown:\")\n",
    "print(f\"  BLEU-1: {corpus_bleu_pangasinan.precisions[0]:.4f}\")\n",
    "print(f\"  BLEU-2: {corpus_bleu_pangasinan.precisions[1]:.4f}\")\n",
    "print(f\"  BLEU-3: {corpus_bleu_pangasinan.precisions[2]:.4f}\")\n",
    "print(f\"  BLEU-4: {corpus_bleu_pangasinan.precisions[3]:.4f}\")\n",
    "print(f\"  Brevity Penalty: {corpus_bleu_pangasinan.bp:.4f}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b557544",
   "metadata": {},
   "source": [
    "## Load Pangasinan Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PNG_VALIDATION_PATH = Path(\"data/validation/png-tgl.tsv\")\n",
    "print(f\"Loading Pangasinan validation data from: {PNG_VALIDATION_PATH}\")\n",
    "png_val_df = pd.read_csv(PNG_VALIDATION_PATH, sep='\\t')\n",
    "\n",
    "png_val_data = []\n",
    "for idx, row in png_val_df.iterrows():\n",
    "    src = row['Pangasinan']\n",
    "    ref = row['Tagalog']\n",
    "    \n",
    "    if pd.isna(src) or pd.isna(ref):\n",
    "        continue\n",
    "    \n",
    "    src_text = str(src).strip()\n",
    "    ref_text = str(ref).strip()\n",
    "    \n",
    "    if src_text and ref_text:\n",
    "        src_cleaned = clean_string(src_text)\n",
    "        ref_cleaned = clean_string(ref_text)\n",
    "        \n",
    "        png_val_data.append({\n",
    "            'src': src_text,\n",
    "            'src_cleaned': src_cleaned,\n",
    "            'ref': ref_text,\n",
    "            'ref_cleaned': ref_cleaned\n",
    "        })\n",
    "\n",
    "print(f\"Prepared {len(png_val_data)} valid Pangasinan validation pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb65264",
   "metadata": {},
   "source": [
    "## Evaluate Base NLLB on Pangasinan→Tagalog Validation Set (Direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base NLLB model for Pangasinan validation evaluation...\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "png_val_translations = []\n",
    "print(f\"Translating {len(png_val_data)} Pangasinan validation sentences with base NLLB model...\")\n",
    "for i, item in enumerate(png_val_data):\n",
    "    if (i + 1) % max(1, len(png_val_data) // 5) == 0:\n",
    "        print(f\"  Progress: {i+1}/{len(png_val_data)}\")\n",
    "    \n",
    "    translation = translate_nllb(\n",
    "        item['src_cleaned'],\n",
    "        base_model,\n",
    "        base_tokenizer,\n",
    "        device,\n",
    "        tgt_lang=\"tgl_Latn\"\n",
    "    )\n",
    "    png_val_translations.append(translation)\n",
    "\n",
    "print(\"Base model Pangasinan validation translation complete\")\n",
    "\n",
    "del base_model\n",
    "gc.collect()\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "png_val_references = [item['ref_cleaned'] for item in png_val_data]\n",
    "bleu_png_val = sacrebleu.BLEU(smooth_method='exp')\n",
    "corpus_bleu_png_val = bleu_png_val.corpus_score(png_val_translations, [png_val_references])\n",
    "png_val_sentence_bleu_scores = [sacrebleu.BLEU(smooth_method='exp').sentence_score(hyp, [ref]).score for hyp, ref in zip(png_val_translations, png_val_references)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASE NLLB MODEL RESULTS - PANGASINAN→TAGALOG VALIDATION (Direct)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: facebook/nllb-200-distilled-600M\")\n",
    "print(f\"Source Language: Pangasinan\")\n",
    "print(f\"Target Language: Tagalog (tgl_Latn)\")\n",
    "print(f\"Test Pairs: {len(png_val_data)}\")\n",
    "print(f\"\\nCorpus BLEU Score: {corpus_bleu_png_val.score:.4f}\")\n",
    "print(f\"Mean Sentence BLEU: {statistics.mean(png_val_sentence_bleu_scores):.4f}\")\n",
    "print(f\"Median Sentence BLEU: {statistics.median(png_val_sentence_bleu_scores):.4f}\")\n",
    "print(f\"\\nBLEU Breakdown:\")\n",
    "print(f\"  BLEU-1: {corpus_bleu_png_val.precisions[0]:.4f}\")\n",
    "print(f\"  BLEU-2: {corpus_bleu_png_val.precisions[1]:.4f}\")\n",
    "print(f\"  BLEU-3: {corpus_bleu_png_val.precisions[2]:.4f}\")\n",
    "print(f\"  BLEU-4: {corpus_bleu_png_val.precisions[3]:.4f}\")\n",
    "print(f\"  Brevity Penalty: {corpus_bleu_png_val.bp:.4f}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c0cfbe",
   "metadata": {},
   "source": [
    "## Save Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e704a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    'Model': [\n",
    "        'Base NLLB (No Fine-tuning)',\n",
    "        'Base NLLB (No Fine-tuning)',\n",
    "        'Base NLLB (No Fine-tuning)',\n",
    "        'Base NLLB (No Fine-tuning)'\n",
    "    ],\n",
    "    'Source Language': [\n",
    "        'Yami',\n",
    "        'Yami',\n",
    "        'Pangasinan',\n",
    "        'Pangasinan'\n",
    "    ],\n",
    "    'Target Language': [\n",
    "        'Tagalog (tgl_Latn)',\n",
    "        'Tagalog (tgl_Latn)',\n",
    "        'Tagalog (tgl_Latn)',\n",
    "        'Tagalog (tgl_Latn)'\n",
    "    ],\n",
    "    'Dataset': [\n",
    "        'Yami Validation Set',\n",
    "        'Yami Bible Dataset',\n",
    "        'Pangasinan Bible Dataset',\n",
    "        'Pangasinan Validation Set'\n",
    "    ],\n",
    "    'Corpus BLEU': [\n",
    "        corpus_bleu_base.score,\n",
    "        corpus_bleu_base_bible.score,\n",
    "        corpus_bleu_pangasinan.score,\n",
    "        corpus_bleu_png_val.score\n",
    "    ],\n",
    "    'Mean BLEU': [\n",
    "        statistics.mean(base_sentence_bleu_scores),\n",
    "        statistics.mean(base_bible_sentence_bleu_scores),\n",
    "        statistics.mean(pangasinan_sentence_bleu_scores),\n",
    "        statistics.mean(png_val_sentence_bleu_scores)\n",
    "    ],\n",
    "    'BLEU-1': [\n",
    "        corpus_bleu_base.precisions[0],\n",
    "        corpus_bleu_base_bible.precisions[0],\n",
    "        corpus_bleu_pangasinan.precisions[0],\n",
    "        corpus_bleu_png_val.precisions[0]\n",
    "    ],\n",
    "    'BLEU-2': [\n",
    "        corpus_bleu_base.precisions[1],\n",
    "        corpus_bleu_base_bible.precisions[1],\n",
    "        corpus_bleu_pangasinan.precisions[1],\n",
    "        corpus_bleu_png_val.precisions[1]\n",
    "    ],\n",
    "    'BLEU-3': [\n",
    "        corpus_bleu_base.precisions[2],\n",
    "        corpus_bleu_base_bible.precisions[2],\n",
    "        corpus_bleu_pangasinan.precisions[2],\n",
    "        corpus_bleu_png_val.precisions[2]\n",
    "    ],\n",
    "    'BLEU-4': [\n",
    "        corpus_bleu_base.precisions[3],\n",
    "        corpus_bleu_base_bible.precisions[3],\n",
    "        corpus_bleu_pangasinan.precisions[3],\n",
    "        corpus_bleu_png_val.precisions[3]\n",
    "    ],\n",
    "    'Brevity Penalty': [\n",
    "        corpus_bleu_base.bp,\n",
    "        corpus_bleu_base_bible.bp,\n",
    "        corpus_bleu_pangasinan.bp,\n",
    "        corpus_bleu_png_val.bp\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_path = Path('results/base_nllb_evaluation.csv')\n",
    "comparison_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASE NLLB RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "print(f\"\\nResults saved to: {comparison_path}\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
